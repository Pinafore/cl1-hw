
This homework is about dependency parsing.  We'll create a classifier to create a shift-reduce parser.


Big Picture
===========

The goal of this homework is to create a dependency parser.  This will be in two stages: first to create training data (an oracle sequence of transitions), and then to train a classifier to produce the same sequence of transitions.

Evaluating the classifier is a bit tricky because the errors are not independent.  If you make a mistake on one word, it will affect the ability to correctly parse the rest of the sentence.  We'll use two metrics: classification accuracy (how often the classifier chooses the correct action given all previous actions were correct, similar to teacher forcing we discussed for LLM decoding) and attachment accuracy (how often the classifier chooses the correct parent for a word, which is extra credit).

Data
===========

First, we’ll need some dependency parsed sentences.  NLTK has a small dataset available:


    >>> from nltk.corpus import dependency_treebank
    >>> s = dependency_treebank.parsed_sents()[0]
    >>> for ii in s.to_conll(4).split("\n"): print(ii)
    ...
    Pierre        NNP        2        
    Vinken        NNP        8        
    ,        ,        2        
    61        CD        5        
    years        NNS        6        
    old        JJ        2        
    ,        ,        2        
    will        MD        0        j
    join        VB        8        
    the        DT        11        
    board        NN        9        
    as        IN        9        
    a        DT        15        
    nonexecutive        JJ        15        
    director        NN        12        
    Nov.        NNP        9        
    29        CD        16        
    .        .        8


What to do
============

0.  Read the code and understand it.  This includes both the
`dependency.py` file and the `test_dependency.py` file.

0. Implement the stack and buffer operations in the class
`ShiftReduceState`.  You will need to implement three functions (one
for each action type): `shift`, `right_arc`, and `left_arc`.

1. Given a DependencyGraph object, create a method (`transition_sequence`) that
produces a series of shift-reduce moves (instances of the `Transition` object)
that produces the tree.  

2. Given a series of shift-reduce moves (as generated by
`transition_sequence`) and an input sentence (the words and tags),
produce the DependencyGraph by implementing the function
`parse_from_transition`.

3. You'll train a classifier to produce the same transitions.  The
first step is to generate feature vectors for your transitions.  Start
with something simple!  *You cannot use the true transitions
or the true edges as features* (that's what you're trying to predict,
so that would be cheating).

4. Given a set of transitions, train a MaxEnt classifier to produce
the same moves using the feature set.  (Use “IIS” first for the
algorithm - I had trouble with others.)  This code is given to you,
you'll just need to get the feature set for it.

What constitutes a "good enough" implementation?
===========================================

Your oracle transition sequence should be correct.  You should be able
to reconstruct the dependency tree from the transition sequence for
any projective tree in the dataset. [10 points]

While it's possible to get over 85% accuracy on the test data, you
just need to get over 75% to get full credit.  (It might be possible
to get above that with more feature engineering.)  [10 points]

What does a good implementation look like?
===========================================

When you run the code, you should see something like this.  It will
first print the sentence, then the oracle transitions.  After that it
will train the classifier and print out the accuracy.

    python3 -i dependency.py 
        s
        l	(news, economic)
        s
        l	(had, news)
        s
        s
        l	(effect, little)
        s
        s
        s
        l	(markets, financial)
        r	(on, markets)
        r	(effect, on)
        r	(had, effect)
        s
        r	(had, .)
        r	(None, had)
        s
        Training data: 3522
        Test data: 392
        ==> Training (25 iterations)

            Iteration    Log Likelihood    Accuracy
            ---------------------------------------
                    1          -1.09861        0.500
                    2          -0.70682        0.650
                    3          -0.58589        0.823
                    4          -0.51217        0.849
                    5          -0.46243        0.861
                    6          -0.42635        0.870
                    7          -0.39876        0.876
                    8          -0.37684        0.882
                    9          -0.35889        0.887
                    10          -0.34386        0.891
                    11          -0.33104        0.894
                    12          -0.31995        0.898
                    13          -0.31022        0.900
                    14          -0.30160        0.903
                    15          -0.29389        0.905
                    16          -0.28695        0.907
                    17          -0.28065        0.909
                    18          -0.27491        0.911
                    19          -0.26964        0.912
                    20          -0.26479        0.914
                    21          -0.26030        0.915
                    22          -0.25612        0.916
                    23          -0.25223        0.918
                    24          -0.24859        0.918
                Final          -0.24518        0.919
        Held-out Classification Accuracy: 0.8631
        Held-out Attachment Accuracy:     0.6411

What to turn in
===============

Submit your code on Gradescope.  Provide a brief writeup explaining your feature engineering.

Extra Credit
===============

1.  [Up to 5 points] Do feature engineering to further improve the
accuracy.  (You cannot change the classifier or dataset.)

2.  [Up to 5 points] Complete the code to compute attachment accuracy.
This requires three things: implement the `attachment_accuracy`
function, implement the `sentence_attachment_accuracy` function, and
use the `classifier_transition_sequence` function to generate the
transitions from the classifier.  You might need to harden some code
elsewhere to account for impossible / incomplete transitions.

3.  [Up to 5 points] Do an error analysis to understand what the
classifier is getting wrong.  What types of transitions does it
mispredict (i.e., what kinds of situations does it get confused by)?

4.  [Up to 5 points] Implement the [RULES
baseline](https://aclanthology.org/W12-1910.pdf) via the function
`heuristic_transition_sequence` and report its accuracy on this
dataset.


FAQ
========

*Q: Why do I not see the transition sequence?*

*A:* Initially, the `transition_sequence` function returns an empty
iterator.  You need to implement the function to generate the
sequence.

*Q: Why do I get a division by zero error when I train the classifier?*

*A:* Because the `transition_sequence` function is returning an empty
iterator, there are no features to train on.

*Q: Whoohoo, I'm getting 100% accuracy!*

*A:* Are you sure you're not using the transition type or the edges
(the things you're trying to predict) as features?
